{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "29be7556-1cb4-4df2-a6f6-9d331345f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_643296/454896261.py:10: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  from scipy.sparse.csc import csc_matrix\n",
      "/tmp/ipykernel_643296/454896261.py:11: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  from scipy.sparse.csr import csr_matrix\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import ot\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse.csc import csc_matrix\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.backends import cudnn\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "    \n",
    "global ret_global\n",
    "global ret_a_global\n",
    "global label_global\n",
    "\n",
    "\n",
    "class GraphST():\n",
    "    def __init__(self, \n",
    "        adata,\n",
    "        adata_sc = None,\n",
    "        device= torch.device('cpu'),\n",
    "        learning_rate=0.001,\n",
    "        learning_rate_sc = 0.01,\n",
    "        weight_decay=0.00,\n",
    "        epochs=600, \n",
    "        dim_input=3000,\n",
    "        dim_output=64,\n",
    "        random_seed = 41,\n",
    "        alpha = 10,\n",
    "        beta = 1,\n",
    "        theta = 0.1,\n",
    "        lamda1 = 10,\n",
    "        lamda2 = 1,\n",
    "        deconvolution = False,\n",
    "        datatype = '10X'\n",
    "        ):\n",
    "        '''\\\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata : anndata\n",
    "            AnnData object of spatial data.\n",
    "        adata_sc : anndata, optional\n",
    "            AnnData object of scRNA-seq data. adata_sc is needed for deconvolution. The default is None.\n",
    "        device : string, optional\n",
    "            Using GPU or CPU? The default is 'cpu'.\n",
    "        learning_rate : float, optional\n",
    "            Learning rate for ST representation learning. The default is 0.001.\n",
    "        learning_rate_sc : float, optional\n",
    "            Learning rate for scRNA representation learning. The default is 0.01.\n",
    "        weight_decay : float, optional\n",
    "            Weight factor to control the influence of weight parameters. The default is 0.00.\n",
    "        epochs : int, optional\n",
    "            Epoch for model training. The default is 600.\n",
    "        dim_input : int, optional\n",
    "            Dimension of input feature. The default is 3000.\n",
    "        dim_output : int, optional\n",
    "            Dimension of output representation. The default is 64.\n",
    "        random_seed : int, optional\n",
    "            Random seed to fix model initialization. The default is 41.\n",
    "        alpha : float, optional\n",
    "            Weight factor to control the influence of reconstruction loss in representation learning. \n",
    "            The default is 10.\n",
    "        beta : float, optional\n",
    "            Weight factor to control the influence of contrastive loss in representation learning. \n",
    "            The default is 1.\n",
    "        lamda1 : float, optional\n",
    "            Weight factor to control the influence of reconstruction loss in mapping matrix learning. \n",
    "            The default is 10.\n",
    "        lamda2 : float, optional\n",
    "            Weight factor to control the influence of contrastive loss in mapping matrix learning. \n",
    "            The default is 1.\n",
    "        deconvolution : bool, optional\n",
    "            Deconvolution task? The default is False.\n",
    "        datatype : string, optional    \n",
    "            Data type of input. Our model supports 10X Visium ('10X'), Stereo-seq ('Stereo'), and Slide-seq/Slide-seqV2 ('Slide') data. \n",
    "        Returns\n",
    "        -------\n",
    "        The learned representation 'self.emb_rec'.\n",
    "\n",
    "        '''\n",
    "        self.adata = adata.copy()\n",
    "        self.device = device\n",
    "        self.learning_rate=learning_rate\n",
    "        self.learning_rate_sc = learning_rate_sc\n",
    "        self.weight_decay=weight_decay\n",
    "        self.epochs=epochs\n",
    "        self.random_seed = random_seed\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.theta = theta\n",
    "        self.lamda1 = lamda1\n",
    "        self.lamda2 = lamda2\n",
    "        self.deconvolution = deconvolution\n",
    "        self.datatype = datatype\n",
    "        \n",
    "        fix_seed(self.random_seed)\n",
    "        \n",
    "        if 'highly_variable' not in adata.var.keys():\n",
    "           preprocess(self.adata)\n",
    "        \n",
    "        if 'adj' not in adata.obsm.keys():\n",
    "           if self.datatype in ['Stereo', 'Slide']:\n",
    "              construct_interaction_KNN(self.adata)\n",
    "           else:    \n",
    "              construct_interaction(self.adata)\n",
    "         \n",
    "        if 'label_CSL' not in adata.obsm.keys():    \n",
    "           add_contrastive_label(self.adata)\n",
    "           \n",
    "        if 'feat' not in adata.obsm.keys():\n",
    "           get_feature(self.adata)\n",
    "        \n",
    "        self.features = torch.FloatTensor(self.adata.obsm['feat'].copy()).to(self.device)\n",
    "        self.features_a = torch.FloatTensor(self.adata.obsm['feat_a'].copy()).to(self.device)\n",
    "        self.label_CSL = torch.FloatTensor(self.adata.obsm['label_CSL']).to(self.device)\n",
    "        self.adj = self.adata.obsm['adj']\n",
    "        self.graph_neigh = torch.FloatTensor(self.adata.obsm['graph_neigh'].copy() + np.eye(self.adj.shape[0])).to(self.device)\n",
    "    \n",
    "        self.dim_input = self.features.shape[1]\n",
    "        self.dim_output = dim_output\n",
    "        \n",
    "        if self.datatype in ['Stereo', 'Slide']:\n",
    "           #using sparse\n",
    "           print('Building sparse matrix ...')\n",
    "           self.adj = preprocess_adj_sparse(self.adj).to(self.device)\n",
    "        else: \n",
    "           # standard version\n",
    "           self.adj = preprocess_adj(self.adj)\n",
    "           self.adj = torch.FloatTensor(self.adj).to(self.device)\n",
    "        \n",
    "        if self.deconvolution:\n",
    "           self.adata_sc = adata_sc.copy() \n",
    "            \n",
    "           if isinstance(self.adata.X, csc_matrix) or isinstance(self.adata.X, csr_matrix):\n",
    "              self.feat_sp = adata.X.toarray()[:, ]\n",
    "           else:\n",
    "              self.feat_sp = adata.X[:, ]\n",
    "           if isinstance(self.adata_sc.X, csc_matrix) or isinstance(self.adata_sc.X, csr_matrix):\n",
    "              self.feat_sc = self.adata_sc.X.toarray()[:, ]\n",
    "           else:\n",
    "              self.feat_sc = self.adata_sc.X[:, ]\n",
    "            \n",
    "           # fill nan as 0\n",
    "           self.feat_sc = pd.DataFrame(self.feat_sc).fillna(0).values\n",
    "           self.feat_sp = pd.DataFrame(self.feat_sp).fillna(0).values\n",
    "          \n",
    "           self.feat_sc = torch.FloatTensor(self.feat_sc).to(self.device)\n",
    "           self.feat_sp = torch.FloatTensor(self.feat_sp).to(self.device)\n",
    "        \n",
    "           if self.adata_sc is not None:\n",
    "              self.dim_input = self.feat_sc.shape[1] \n",
    "\n",
    "           self.n_cell = adata_sc.n_obs\n",
    "           self.n_spot = adata.n_obs\n",
    "            \n",
    "    def train(self):\n",
    "        if self.datatype in ['Stereo', 'Slide']:\n",
    "           self.model = Encoder_sparse(self.dim_input, self.dim_output, self.graph_neigh).to(self.device)\n",
    "        else:\n",
    "           self.model = Encoder(self.dim_input, self.dim_output, self.graph_neigh).to(self.device)\n",
    "        self.loss_CSL = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), self.learning_rate, \n",
    "                                          weight_decay=self.weight_decay)\n",
    "        \n",
    "        print('Begin to train ST data...')\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in tqdm(range(self.epochs)): \n",
    "            self.model.train()\n",
    "              \n",
    "            self.features_a = permutation(self.features)\n",
    "            self.hiden_feat, self.emb, ret, ret_a, self.g = self.model(self.features, self.features_a, self.adj)\n",
    "        \n",
    "            self.loss_sl_1 = self.loss_CSL(ret, self.label_CSL)\n",
    "            self.loss_sl_2 = self.loss_CSL(ret_a, self.label_CSL)\n",
    "            self.loss_feat = F.mse_loss(self.features, self.emb)\n",
    "\n",
    "            self.ret = ret\n",
    "            self.ret_a = ret_a\n",
    "\n",
    "            raise ValueError\n",
    "                                    \n",
    "            loss =  self.alpha*self.loss_feat + self.beta*(self.loss_sl_1 + self.loss_sl_2)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward() \n",
    "            self.optimizer.step()\n",
    "        \n",
    "        print(\"Optimization finished for ST data!\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "             self.model.eval()\n",
    "             if self.deconvolution:\n",
    "                self.emb_rec = self.model(self.features, self.features_a, self.adj)[1]\n",
    "                \n",
    "                return self.emb_rec\n",
    "             else:  \n",
    "                if self.datatype in ['Stereo', 'Slide']:\n",
    "                   self.emb_rec = self.model(self.features, self.features_a, self.adj)[1]\n",
    "                   self.emb_rec = F.normalize(self.emb_rec, p=2, dim=1).detach().cpu().numpy() \n",
    "                else:\n",
    "                   self.emb_rec = self.model(self.features, self.features_a, self.adj)[1].detach().cpu().numpy()\n",
    "                self.adata.obsm['emb'] = self.emb_rec\n",
    "                \n",
    "                return self.adata\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = c.expand_as(h_pl)  \n",
    "\n",
    "        sc_1 = self.f_k(h_pl, c_x)\n",
    "        sc_2 = self.f_k(h_mi, c_x)\n",
    "\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "\n",
    "        logits = torch.cat((sc_1, sc_2), 1)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "class AvgReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgReadout, self).__init__()\n",
    "\n",
    "    def forward(self, emb, mask=None):\n",
    "        vsum = torch.mm(mask, emb)\n",
    "        row_sum = torch.sum(mask, 1)\n",
    "        row_sum = row_sum.expand((vsum.shape[1], row_sum.shape[0])).T\n",
    "        global_emb = vsum / row_sum \n",
    "          \n",
    "        return F.normalize(global_emb, p=2, dim=1) \n",
    "    \n",
    "class Encoder(Module):\n",
    "    def __init__(self, in_features, out_features, graph_neigh, dropout=0.0, act=F.relu):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.graph_neigh = graph_neigh\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        \n",
    "        self.weight1 = Parameter(torch.FloatTensor(self.in_features, self.out_features))\n",
    "        self.weight2 = Parameter(torch.FloatTensor(self.out_features, self.in_features))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        self.disc = Discriminator(self.out_features)\n",
    "\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.read = AvgReadout()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight1)\n",
    "        torch.nn.init.xavier_uniform_(self.weight2)\n",
    "\n",
    "    def forward(self, feat, feat_a, adj):\n",
    "        z = F.dropout(feat, self.dropout, self.training)\n",
    "        z = torch.mm(z, self.weight1)\n",
    "        z = torch.mm(adj, z)\n",
    "        \n",
    "        hiden_emb = z\n",
    "        \n",
    "        h = torch.mm(z, self.weight2)\n",
    "        h = torch.mm(adj, h)\n",
    "        \n",
    "        emb = self.act(z)\n",
    "        \n",
    "        z_a = F.dropout(feat_a, self.dropout, self.training)\n",
    "        z_a = torch.mm(z_a, self.weight1)\n",
    "        z_a = torch.mm(adj, z_a)\n",
    "        emb_a = self.act(z_a)\n",
    "        \n",
    "        g = self.read(emb, self.graph_neigh) \n",
    "        g = self.sigm(g) \n",
    "\n",
    "        g_a = self.read(emb_a, self.graph_neigh)\n",
    "        g_a = self.sigm(g_a) \n",
    "\n",
    "        print(g, g.shape)\n",
    "        print(g_a, g_a.shape)\n",
    "        \n",
    "        ret = self.disc(g, emb, emb_a)  \n",
    "        ret_a = self.disc(g_a, emb_a, emb) \n",
    "        \n",
    "        return hiden_emb, h, ret, ret_a, g\n",
    "\n",
    "def filter_with_overlap_gene(adata, adata_sc):\n",
    "    # remove all-zero-valued genes\n",
    "    #sc.pp.filter_genes(adata, min_cells=1)\n",
    "    #sc.pp.filter_genes(adata_sc, min_cells=1)\n",
    "    \n",
    "    if 'highly_variable' not in adata.var.keys():\n",
    "       raise ValueError(\"'highly_variable' are not existed in adata!\")\n",
    "    else:    \n",
    "       adata = adata[:, adata.var['highly_variable']]\n",
    "       \n",
    "    if 'highly_variable' not in adata_sc.var.keys():\n",
    "       raise ValueError(\"'highly_variable' are not existed in adata_sc!\")\n",
    "    else:    \n",
    "       adata_sc = adata_sc[:, adata_sc.var['highly_variable']]   \n",
    "\n",
    "    # Refine `marker_genes` so that they are shared by both adatas\n",
    "    genes = list(set(adata.var.index) & set(adata_sc.var.index))\n",
    "    genes.sort()\n",
    "    print('Number of overlap genes:', len(genes))\n",
    "\n",
    "    adata.uns[\"overlap_genes\"] = genes\n",
    "    adata_sc.uns[\"overlap_genes\"] = genes\n",
    "    \n",
    "    adata = adata[:, genes]\n",
    "    adata_sc = adata_sc[:, genes]\n",
    "    \n",
    "    return adata, adata_sc\n",
    "\n",
    "def permutation(feature):\n",
    "    # fix_seed(FLAGS.random_seed) \n",
    "    ids = np.arange(feature.shape[0])\n",
    "    ids = np.random.permutation(ids)\n",
    "    feature_permutated = feature[ids]\n",
    "    \n",
    "    return feature_permutated \n",
    "\n",
    "def construct_interaction(adata, n_neighbors=3):\n",
    "    \"\"\"Constructing spot-to-spot interactive graph\"\"\"\n",
    "    position = adata.obsm['spatial']\n",
    "    \n",
    "    # calculate distance matrix\n",
    "    distance_matrix = ot.dist(position, position, metric='euclidean')\n",
    "    n_spot = distance_matrix.shape[0]\n",
    "    \n",
    "    adata.obsm['distance_matrix'] = distance_matrix\n",
    "    \n",
    "    # find k-nearest neighbors\n",
    "    interaction = np.zeros([n_spot, n_spot])  \n",
    "    for i in range(n_spot):\n",
    "        vec = distance_matrix[i, :]\n",
    "        distance = vec.argsort()\n",
    "        for t in range(1, n_neighbors + 1):\n",
    "            y = distance[t]\n",
    "            interaction[i, y] = 1\n",
    "         \n",
    "    adata.obsm['graph_neigh'] = interaction\n",
    "    \n",
    "    #transform adj to symmetrical adj\n",
    "    adj = interaction\n",
    "    adj = adj + adj.T\n",
    "    adj = np.where(adj>1, 1, adj)\n",
    "    \n",
    "    adata.obsm['adj'] = adj\n",
    "    \n",
    "def construct_interaction_KNN(adata, n_neighbors=3):\n",
    "    position = adata.obsm['spatial']\n",
    "    n_spot = position.shape[0]\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors+1).fit(position)  \n",
    "    _ , indices = nbrs.kneighbors(position)\n",
    "    x = indices[:, 0].repeat(n_neighbors)\n",
    "    y = indices[:, 1:].flatten()\n",
    "    interaction = np.zeros([n_spot, n_spot])\n",
    "    interaction[x, y] = 1\n",
    "    \n",
    "    adata.obsm['graph_neigh'] = interaction\n",
    "    \n",
    "    #transform adj to symmetrical adj\n",
    "    adj = interaction\n",
    "    adj = adj + adj.T\n",
    "    adj = np.where(adj>1, 1, adj)\n",
    "    \n",
    "    adata.obsm['adj'] = adj\n",
    "    print('Graph constructed!')   \n",
    "\n",
    "def preprocess(adata):\n",
    "    sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=3000)\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.scale(adata, zero_center=False, max_value=10)\n",
    "    \n",
    "def get_feature(adata, deconvolution=False):\n",
    "    if deconvolution:\n",
    "       adata_Vars = adata\n",
    "    else:   \n",
    "       adata_Vars =  adata[:, adata.var['highly_variable']]\n",
    "       \n",
    "    if isinstance(adata_Vars.X, csc_matrix) or isinstance(adata_Vars.X, csr_matrix):\n",
    "       feat = adata_Vars.X.toarray()[:, ]\n",
    "    else:\n",
    "       feat = adata_Vars.X[:, ] \n",
    "    \n",
    "    # data augmentation\n",
    "    feat_a = permutation(feat)\n",
    "    \n",
    "    adata.obsm['feat'] = feat\n",
    "    adata.obsm['feat_a'] = feat_a    \n",
    "    \n",
    "def add_contrastive_label(adata):\n",
    "    # contrastive label\n",
    "    n_spot = adata.n_obs\n",
    "    one_matrix = np.ones([n_spot, 1])\n",
    "    zero_matrix = np.zeros([n_spot, 1])\n",
    "    label_CSL = np.concatenate([one_matrix, zero_matrix], axis=1)\n",
    "    adata.obsm['label_CSL'] = label_CSL\n",
    "    \n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "    return adj.toarray()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj)+np.eye(adj.shape[0])\n",
    "    return adj_normalized \n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def preprocess_adj_sparse(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_mx_to_torch_sparse_tensor(adj_normalized)    \n",
    "\n",
    "def fix_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9f929adc-c975-43c6-a5f3-7bb479d3b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datadir = '/data1/shahs3/users/mezallj1/data/dlpfc'#dataset path\n",
    "\n",
    "# the location of R, which is necessary for mclust algorithm. Please replace it with local R installation path\n",
    "os.environ['R_HOME'] ='/home/mezallj1/miniconda3/envs/graphst/lib/R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "81b863b1-d0fd-4ce9-8c37-976da3a1368a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 3639 × 33538\n",
       "    obs: 'in_tissue', 'array_row', 'array_col', 'Region'\n",
       "    var: 'gene_ids', 'feature_types', 'genome'\n",
       "    uns: 'spatial'\n",
       "    obsm: 'spatial'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = sc.read_h5ad(f'{datadir}/raw/151673.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8a3132f7-3c19-44de-8383-b7bf54b77ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of clusters\n",
    "n_clusters = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "18036c97-b475-4c45-999c-c1e24c55bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6c3e94d9-ad18-4e4b-9978-01bc2d286b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to train ST data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5252, 0.5000, 0.5006,  ..., 0.5000, 0.5615, 0.5000],\n",
      "        [0.5016, 0.5054, 0.5047,  ..., 0.5195, 0.5000, 0.5000],\n",
      "        [0.5150, 0.5000, 0.5210,  ..., 0.5150, 0.5436, 0.5008],\n",
      "        ...,\n",
      "        [0.5179, 0.5000, 0.5000,  ..., 0.5022, 0.5411, 0.5000],\n",
      "        [0.5093, 0.5000, 0.5074,  ..., 0.5199, 0.5472, 0.5000],\n",
      "        [0.5089, 0.5000, 0.5000,  ..., 0.5022, 0.5656, 0.5000]],\n",
      "       grad_fn=<SigmoidBackward0>) torch.Size([3639, 64])\n",
      "tensor([[0.5016, 0.5000, 0.5000,  ..., 0.5306, 0.5500, 0.5000],\n",
      "        [0.5148, 0.5000, 0.5000,  ..., 0.5242, 0.5318, 0.5000],\n",
      "        [0.5294, 0.5000, 0.5012,  ..., 0.5000, 0.5210, 0.5000],\n",
      "        ...,\n",
      "        [0.5112, 0.5000, 0.5019,  ..., 0.5085, 0.5348, 0.5000],\n",
      "        [0.5284, 0.5000, 0.5058,  ..., 0.5017, 0.5546, 0.5000],\n",
      "        [0.5219, 0.5000, 0.5212,  ..., 0.5148, 0.5998, 0.5000]],\n",
      "       grad_fn=<SigmoidBackward0>) torch.Size([3639, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# define and train model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GraphST(adata)\n\u001b[0;32m----> 3\u001b[0m adata \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[189], line 190\u001b[0m, in \u001b[0;36mGraphST.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mret \u001b[38;5;241m=\u001b[39m ret\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mret_a \u001b[38;5;241m=\u001b[39m ret_a\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[1;32m    192\u001b[0m loss \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_feat \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_sl_1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_sl_2)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define and train model\n",
    "model = GraphST(adata)\n",
    "adata = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5c0fb95b-9a15-4f70-8ae0-9bc5c8a186c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  11.,   80.,  347.,  928., 1169.,  740.,  291.,   60.,   10.,\n",
       "            3.],\n",
       "        [   9.,   55.,  393.,  914., 1172.,  765.,  261.,   60.,   10.,\n",
       "            0.]]),\n",
       " array([-2.12799454, -1.66143131, -1.19486809, -0.72830486, -0.26174164,\n",
       "         0.20482159,  0.67138481,  1.13794804,  1.60451126,  2.07107449,\n",
       "         2.53763771]),\n",
       " <a list of 2 BarContainer objects>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model.ret.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "48e6c86e-debd-464a-8475-c7163b8412d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.055e+03, 9.700e+02, 3.820e+02, 1.680e+02, 5.500e+01, 4.000e+00,\n",
       "         5.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.637e+03, 2.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.973e+03, 5.190e+02, 1.230e+02, 2.200e+01, 2.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.449e+03, 1.540e+02, 2.700e+01, 9.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [5.810e+02, 9.650e+02, 9.940e+02, 7.530e+02, 2.630e+02, 6.500e+01,\n",
       "         1.800e+01, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.397e+03, 1.197e+03, 7.410e+02, 2.470e+02, 4.900e+01, 8.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.620e+02, 7.480e+02, 1.137e+03, 9.420e+02, 4.190e+02, 1.120e+02,\n",
       "         1.700e+01, 2.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.475e+03, 1.430e+02, 1.600e+01, 5.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.521e+03, 1.050e+02, 1.300e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.471e+03, 1.420e+02, 2.600e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.199e+03, 9.290e+02, 3.630e+02, 1.220e+02, 2.200e+01, 4.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.342e+03, 2.210e+02, 6.200e+01, 1.300e+01, 1.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.476e+03, 1.103e+03, 6.680e+02, 2.950e+02, 8.600e+01, 1.100e+01,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.829e+03, 1.137e+03, 5.120e+02, 1.440e+02, 1.700e+01, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.442e+03, 1.099e+03, 7.610e+02, 2.740e+02, 5.900e+01, 4.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.971e+03, 5.240e+02, 1.310e+02, 1.200e+01, 1.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [4.680e+02, 7.840e+02, 9.660e+02, 8.670e+02, 3.890e+02, 1.280e+02,\n",
       "         3.200e+01, 5.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.306e+03, 9.420e+02, 3.040e+02, 6.900e+01, 1.200e+01, 6.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.668e+03, 7.000e+02, 2.070e+02, 4.600e+01, 1.600e+01, 2.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.480e+02, 6.080e+02, 8.310e+02, 9.860e+02, 6.030e+02, 2.120e+02,\n",
       "         4.800e+01, 3.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.632e+03, 7.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.568e+03, 5.500e+01, 1.300e+01, 3.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.612e+03, 2.700e+01, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.512e+03, 8.900e+01, 2.300e+01, 1.100e+01, 4.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.582e+03, 5.000e+01, 6.000e+00, 1.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.633e+03, 6.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.428e+03, 7.580e+02, 2.870e+02, 1.080e+02, 4.900e+01, 9.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.624e+03, 1.500e+01, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.625e+03, 1.400e+01, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.876e+03, 5.720e+02, 1.410e+02, 3.900e+01, 1.100e+01, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.311e+03, 1.259e+03, 7.170e+02, 2.670e+02, 5.300e+01, 2.500e+01,\n",
       "         2.000e+00, 2.000e+00, 1.000e+00, 2.000e+00],\n",
       "        [2.559e+03, 7.860e+02, 2.270e+02, 4.800e+01, 1.900e+01, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.625e+03, 1.400e+01, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.257e+03, 9.050e+02, 3.550e+02, 1.040e+02, 1.200e+01, 6.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [9.710e+02, 1.217e+03, 9.390e+02, 4.020e+02, 1.030e+02, 5.000e+00,\n",
       "         2.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.700e+01, 1.650e+02, 4.830e+02, 9.130e+02, 9.550e+02, 6.910e+02,\n",
       "         2.790e+02, 1.170e+02, 7.000e+00, 2.000e+00],\n",
       "        [2.099e+03, 1.035e+03, 3.740e+02, 1.160e+02, 1.300e+01, 2.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.584e+03, 1.184e+03, 6.530e+02, 1.960e+02, 2.200e+01, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [5.770e+02, 1.038e+03, 9.800e+02, 6.870e+02, 2.710e+02, 7.100e+01,\n",
       "         7.000e+00, 7.000e+00, 1.000e+00, 0.000e+00],\n",
       "        [1.766e+03, 1.119e+03, 5.400e+02, 1.640e+02, 4.600e+01, 4.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.117e+03, 9.280e+02, 4.220e+02, 1.460e+02, 2.400e+01, 2.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.100e+01, 1.010e+02, 3.580e+02, 6.390e+02, 9.540e+02, 9.630e+02,\n",
       "         4.650e+02, 1.130e+02, 2.300e+01, 2.000e+00],\n",
       "        [3.630e+03, 9.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [0.000e+00, 8.000e+00, 7.800e+01, 2.930e+02, 7.560e+02, 1.173e+03,\n",
       "         9.580e+02, 3.280e+02, 4.400e+01, 1.000e+00],\n",
       "        [3.550e+03, 7.700e+01, 1.200e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [9.120e+02, 1.191e+03, 9.260e+02, 4.640e+02, 1.280e+02, 1.600e+01,\n",
       "         2.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [5.590e+02, 1.014e+03, 1.088e+03, 6.860e+02, 2.400e+02, 4.900e+01,\n",
       "         3.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.639e+03, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.633e+03, 5.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.906e+03, 9.690e+02, 5.550e+02, 1.620e+02, 4.600e+01, 1.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.367e+03, 2.200e+02, 4.800e+01, 4.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.631e+03, 6.000e+00, 2.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.574e+03, 6.300e+01, 2.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [6.260e+02, 9.930e+02, 8.520e+02, 5.830e+02, 3.150e+02, 1.650e+02,\n",
       "         8.500e+01, 2.000e+01, 0.000e+00, 0.000e+00],\n",
       "        [2.682e+03, 7.360e+02, 1.900e+02, 3.000e+01, 1.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.620e+02, 6.300e+02, 7.850e+02, 8.960e+02, 6.680e+02, 2.290e+02,\n",
       "         6.600e+01, 3.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.135e+03, 9.930e+02, 3.830e+02, 1.080e+02, 1.700e+01, 3.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.426e+03, 8.140e+02, 3.050e+02, 7.700e+01, 1.700e+01, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.920e+02, 6.180e+02, 1.018e+03, 9.650e+02, 5.530e+02, 2.640e+02,\n",
       "         2.600e+01, 2.000e+00, 1.000e+00, 0.000e+00],\n",
       "        [1.029e+03, 1.158e+03, 8.710e+02, 4.150e+02, 1.280e+02, 3.700e+01,\n",
       "         1.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.339e+03, 2.570e+02, 3.600e+01, 4.000e+00, 3.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [2.122e+03, 9.830e+02, 3.910e+02, 1.200e+02, 2.200e+01, 1.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [1.400e+02, 4.980e+02, 9.190e+02, 1.072e+03, 7.320e+02, 2.370e+02,\n",
       "         3.600e+01, 5.000e+00, 0.000e+00, 0.000e+00],\n",
       "        [3.632e+03, 7.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n",
       "         0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00]]),\n",
       " array([0.5       , 0.51636684, 0.53273368, 0.54910052, 0.56546736,\n",
       "        0.5818342 , 0.59820104, 0.61456788, 0.63093472, 0.64730155,\n",
       "        0.66366839]),\n",
       " <a list of 64 BarContainer objects>)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(model.g.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "073220e2-42ab-445a-935f-b47e0d40d827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5604,  0.0381],\n",
      "        [-0.4620, -0.1766],\n",
      "        [ 0.8396,  0.3410],\n",
      "        ...,\n",
      "        [ 0.7322,  0.3061],\n",
      "        [ 0.4570, -1.0281],\n",
      "        [ 0.2451, -0.4176]], grad_fn=<CatBackward0>) tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        ...,\n",
      "        [1., 0.],\n",
      "        [1., 0.],\n",
      "        [1., 0.]]) torch.Size([3639, 2]) torch.Size([3639, 2]) cpu cpu torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(model.ret, model.label_CSL, model.ret.shape, model.label_CSL.shape, model.ret.device, model.label_CSL.device, model.ret.dtype, model.label_CSL.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a1c89bcf-6a36-4212-b7e0-90107a3ba8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.ret.detach().cpu().numpy() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865b129-2025-4891-b9f1-2302c202eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set radius to specify the number of neighbors considered during refinement\n",
    "radius = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7327c85-7035-4190-aa89-231ba457aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "from GraphST.utils import clustering\n",
    "clustering(adata, n_clusters, radius=radius, refinement=False) #For DLPFC dataset, we use optional refinement step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d97490-1149-45b7-baad-19ecf23733d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = adata.obs.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b8f99d-8ad3-45b6-8022-6d97254f2ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6342579253141447\n"
     ]
    }
   ],
   "source": [
    "ari = metrics.adjusted_rand_score(obs_df['domain'], obs_df['Region'])\n",
    "print(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ab2e2-cb6c-4215-ad97-f1523991819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c0cfe79e-7bdd-4b74-ba39-623ee0ae0fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0034)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss_CSL(torch.tensor([5.,10.]),torch.tensor([1.,0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f8dabef3-33c6-4088-93cf-9b701f0739ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.,0.]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9a367306-2faa-4d4b-b955-94cb074c848e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graphst/lib/python3.8/site-packages/torch/nn/modules/loss.py:724\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.__init__\u001b[0;34m(self, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, size_average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    723\u001b[0m              pos_weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m, weight)\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_weight\u001b[39m\u001b[38;5;124m'\u001b[39m, pos_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphst/lib/python3.8/site-packages/torch/nn/modules/loss.py:23\u001b[0m, in \u001b[0;36m_Loss.__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy_get_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n",
      "File \u001b[0;32m~/miniconda3/envs/graphst/lib/python3.8/site-packages/torch/nn/_reduction.py:35\u001b[0m, in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     reduce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mand\u001b[39;00m reduce:\n\u001b[1;32m     36\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "nn.BCEWithLogitsLoss(torch.tensor([5.,10.]),torch.tensor([1.,0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c28a9015-219b-4498-aaba-8229929e162b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'BCEWithLogitsLoss' has no attribute '__file__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBCEWithLogitsLoss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__file__\u001b[39;49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'BCEWithLogitsLoss' has no attribute '__file__'"
     ]
    }
   ],
   "source": [
    "nn.BCEWithLogitsLoss.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a672b412-4643-408d-b74d-46ac08d86b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m      \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_CSL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           BCEWithLogitsLoss\n",
       "\u001b[0;31mString form:\u001b[0m    BCEWithLogitsLoss()\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/graphst/lib/python3.8/site-packages/torch/nn/modules/loss.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "This loss combines a `Sigmoid` layer and the `BCELoss` in one single\n",
       "class. This version is more numerically stable than using a plain `Sigmoid`\n",
       "followed by a `BCELoss` as, by combining the operations into one layer,\n",
       "we take advantage of the log-sum-exp trick for numerical stability.\n",
       "\n",
       "The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n",
       "\n",
       ".. math::\n",
       "    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
       "    l_n = - w_n \\left[ y_n \\cdot \\log \\sigma(x_n)\n",
       "    + (1 - y_n) \\cdot \\log (1 - \\sigma(x_n)) \\right],\n",
       "\n",
       "where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n",
       "(default ``'mean'``), then\n",
       "\n",
       ".. math::\n",
       "    \\ell(x, y) = \\begin{cases}\n",
       "        \\operatorname{mean}(L), & \\text{if reduction} = \\text{`mean';}\\\\\n",
       "        \\operatorname{sum}(L),  & \\text{if reduction} = \\text{`sum'.}\n",
       "    \\end{cases}\n",
       "\n",
       "This is used for measuring the error of a reconstruction in for example\n",
       "an auto-encoder. Note that the targets `t[i]` should be numbers\n",
       "between 0 and 1.\n",
       "\n",
       "It's possible to trade off recall and precision by adding weights to positive examples.\n",
       "In the case of multi-label classification the loss can be described as:\n",
       "\n",
       ".. math::\n",
       "    \\ell_c(x, y) = L_c = \\{l_{1,c},\\dots,l_{N,c}\\}^\\top, \\quad\n",
       "    l_{n,c} = - w_{n,c} \\left[ p_c y_{n,c} \\cdot \\log \\sigma(x_{n,c})\n",
       "    + (1 - y_{n,c}) \\cdot \\log (1 - \\sigma(x_{n,c})) \\right],\n",
       "\n",
       "where :math:`c` is the class number (:math:`c > 1` for multi-label binary classification,\n",
       ":math:`c = 1` for single-label binary classification),\n",
       ":math:`n` is the number of the sample in the batch and\n",
       ":math:`p_c` is the weight of the positive answer for the class :math:`c`.\n",
       "\n",
       ":math:`p_c > 1` increases the recall, :math:`p_c < 1` increases the precision.\n",
       "\n",
       "For example, if a dataset contains 100 positive and 300 negative examples of a single class,\n",
       "then ``pos_weight`` for the class should be equal to :math:`\\frac{300}{100}=3`.\n",
       "The loss would act as if the dataset contains :math:`3\\times 100=300` positive examples.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10\n",
       "    >>> output = torch.full([10, 64], 1.5)  # A prediction (logit)\n",
       "    >>> pos_weight = torch.ones([64])  # All weights are equal to 1\n",
       "    >>> criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
       "    >>> criterion(output, target)  # -log(sigmoid(1.5))\n",
       "    tensor(0.20...)\n",
       "\n",
       "In the above example, the ``pos_weight`` tensor's elements correspond to the 64 distinct classes\n",
       "in a multi-label binary classification scenario. Each element in ``pos_weight`` is designed to adjust the\n",
       "loss function based on the imbalance between negative and positive samples for the respective class.\n",
       "This approach is useful in datasets with varying levels of class imbalance, ensuring that the loss\n",
       "calculation accurately accounts for the distribution in each class.\n",
       "\n",
       "Args:\n",
       "    weight (Tensor, optional): a manual rescaling weight given to the loss\n",
       "        of each batch element. If given, has to be a Tensor of size `nbatch`.\n",
       "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "        the losses are averaged over each loss element in the batch. Note that for\n",
       "        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
       "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "        when :attr:`reduce` is ``False``. Default: ``True``\n",
       "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
       "        losses are averaged or summed over observations for each minibatch depending\n",
       "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
       "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
       "    reduction (str, optional): Specifies the reduction to apply to the output:\n",
       "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
       "        ``'mean'``: the sum of the output will be divided by the number of\n",
       "        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
       "        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
       "        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
       "    pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target.\n",
       "        Must be a tensor with equal size along the class dimension to the number of classes.\n",
       "        Pay close attention to PyTorch's broadcasting semantics in order to achieve the desired\n",
       "        operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\n",
       "        size [B, C, H, W] will apply different pos_weights to each element of the batch or\n",
       "        [C, H, W] the same pos_weights across the batch. To apply the same positive weight\n",
       "        along all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\n",
       "        Default: ``None``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
       "    - Target: :math:`(*)`, same shape as the input.\n",
       "    - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(*)`, same\n",
       "      shape as input.\n",
       "\n",
       " Examples::\n",
       "\n",
       "    >>> loss = nn.BCEWithLogitsLoss()\n",
       "    >>> input = torch.randn(3, requires_grad=True)\n",
       "    >>> target = torch.empty(3).random_(2)\n",
       "    >>> output = loss(input, target)\n",
       "    >>> output.backward()\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.loss_CSL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d55c49-9ba1-4707-9ab7-e2f1b1c3c62a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphst",
   "language": "python",
   "name": "graphst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
